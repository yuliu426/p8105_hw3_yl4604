---
title: "p8105_hw3_yl4604"
author: "Yu"
date: "October 6, 2020"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
	fig.width = 6, 
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

##Problem 1 

```{r}
data('instacart')
```


The dataset contains over 3 million online grocery orders information from more than 200,000 Instacart users. The data set has `r nrow(instacart)` rows and `r ncol(instacart)` columns. The dataset has user/order variables, like user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes.  

How many aisles are there, and which aisles are the most items ordered from?

```{r}
aisle_df = 
  instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

There are `r nrow(aisle_df)` aisles in the dataset, and 'fresh vegetables' is where the most items ordered from.

Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.

make a plot!

```{r}
aisle_items = 
  instacart %>% 
  group_by(aisle) %>% 
  summarize(n = n()) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  )
ggplot(aisle_items, aes(x = aisle, y = n)) +
geom_point() +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.

make a table!

```{r}
instacart %>% 
  filter(aisle == c('baking ingredients', 'dog food care', 'packaged vegetables fruits')) %>% 
  group_by(aisle,product_name) %>% 
  summarise(n = n()) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```


Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table)

make another table!

```{r}
instacart %>% 
  filter(product_name == c('Pink Lady Apples', 'Coffee Ice Cream')) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hr = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hr
  ) %>% 
  knitr::kable()
```



## Problem 2

import and tidy the data

```{r}
accel_df = 
  read_csv('./data/accel_data.csv') %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = 'min_of_day',
    values_to = 'amt_act',
    names_prefix = 'activity_'
  ) %>% 
  mutate(min_of_day = as.numeric(min_of_day)) %>% 
  mutate(weekday_weekend = case_when(
    day == 'Saturday' ~ 'weekend',
    day == 'Sunday' ~ 'weekend',
    TRUE ~ 'weekday'
  ) ) %>% 
  mutate(day = forcats::fct_relevel(day, 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'))
  
```

The dataset has variables: `r names(accel_df)`, containing five weeks of the activity counts for each minutes of a 24-hours day of a a 63 year-old male with BMI 25.

Aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals.

```{r}
act_amt_df =
  accel_df %>% 
    group_by(week, day) %>% 
    summarise(total_act_amt = sum(amt_act, na.rm = TRUE)) 
knitr::kable(act_amt_df)

ggplot(act_amt_df, aes(x = week, y = total_act_amt, color = day)) +
geom_point() +
geom_line()

```

???trends?

Make a plot
```{r}
accel_df %>%
  ggplot(aes(x = min_of_day, y = amt_act, color = day)) +
  geom_point(alpha = .5) +
  geom_smooth()
  #geom_line(alpha = .5)
  
```

???


##Problem 3 

```{r}
library(p8105.datasets)
data("ny_noaa")
```

This dataset contains variables for all NEW YORK weather station from January 1, 1981 through December 31, 2010. variables in this datasets are `r names(ny_noaa)`. The dataset has `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns.

Clean and wrangling the data. Find the most commonly obvserved value for snow fall.

```{r}
ny_noaa_df =
  ny_noaa %>% 
  separate(date, into = c('year', 'month', 'day'), sep = '-') %>% 
#? can not use unique(ny_noaa_df, snow)?....object 'snow' not found
  mutate(
    tmin = as.numeric(tmin),
    tmin = tmin / 10,
    tmax = as.numeric(tmax),
    tmax = tmax / 10,
    snow = snow / 10
  )
ny_noaa_df %>% 
  count(snow) %>% 
  arrange(desc(n))
```

The most commonly observed values for snowfall are o and NA. It is because except for some coldest days in winter, normally New York would not snow.

Make a two-panel plot showing the average max temperature in January and in July in each station across years. 

```{r}
ny_noaa_df %>% 
  filter(month %in% c('01','07')) %>% 
  group_by(month, year, id) %>% 
  summarize(avg_tmax = mean(tmax)) %>% 
  ggplot(aes(x = id, y = avg_tmax, color = year)) +
  geom_point()
  geom_line() +
  facet_grid(. ~ month) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

According to the boxplot, average max temperatures in July are significantly higher than  January in each station across years, which is faily reasonalble. There are some outliers.  

tmax vs tmin

```{r}
ny_noaa_df %>% 
  
```

